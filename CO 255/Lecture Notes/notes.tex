\documentclass[11pt]{article}
\usepackage{notes}

\newcommand{\conv}[1]{\text{conv}(#1)}

\newcommand{\thiscoursecode}{CO 255}
\newcommand{\thiscoursename}{Introduction to Optimization (Advanced)}
\newcommand{\thisprof}{Jim Geelen}
\newcommand{\me}{Kevin Cheng}
\newcommand{\thisterm}{Fall 2018}

\newcommand{\proj}[2][]{\textit{proj}_{#1}#2}

\hypersetup
{pdfauthor={\me},
pdftitle={\thiscoursecode \thisterm Lecutre Notes},
pdfsubject={\thiscoursename}
pdflang={English}}

\begin{document}

\begin{titlepage}
\begin{centering}
{\scshape\LARGE University of Waterloo \par}
\globe
{\huge\bf \thiscoursecode}\\
{\scshape\Large \thiscoursename}\\
\vspace{.3cm}
{\scshape Prof. \thisprof~\textbullet~\thisterm\par}
\end{centering}
\sectionline
\tableofcontents
\sectionline
\thispagestyle{empty}
\end{titlepage}

\pagebreak
\section{Introduction}
\subsection{Types of Optimization Problems}
Given a set $S$ and an objective function $f: S \to \R$, an optimization problem
looks like,
\begin{equation*}
    \max_{\text{subject to (s.t.)}} f(x)_{x \in S}
\end{equation*}
which means to find some $x* \in S$ such that $f(x) \leq f(x*)$ for all $x \in
S$. Here, $S$ is called the ``feasible region'', and a point $\bar x \in S$ is
called a ``feasible solution'' and $f(\bar x)$ is called the ``objective
value''.
\begin{definition}
For some feasible region $S$, $x*$ is an \underline{optimal solution} if for all
$x \in S$ that $f(x) \leq f(x*)$.
\end{definition}
We can use different notation like
\begin{equation*}
    \max \{f(x) : x\in S \}
\end{equation*}
or
\begin{equation*}
    \max_{x \in S} f(x).
\end{equation*}
Also, there is a correspondence with \underline{minimization problems} as,
\begin{equation*}
    \max_{x \in S} f(x) = -\bigg( \min_{x \in S} -f(x) \bigg) 
\end{equation*}
A number of problems can arise,
\begin{itemize}
    \item $S = \emptyset$ (This problem is always \underline{infeasible})
	\item If for all $a \in \R$, there always exists some $\bar x \in S: f(\bar
		x) > a$ (This problem is \underline{unbounded})
    \item $\max_{x < 1} x$ (Here, an optimal solution does not exist)
\end{itemize}
\begin{definition}
    A \underline{supremum} is defined as \begin{equation*}
\sup\{f(x):x \in S\} =
    \begin{cases}
        - \infty,& \text{, if infeasible}\\
        + \infty,& \text{, if unbouded}\\
		\min_{f(x) \leq \lambda,\>\forall x \in S}\lambda &\text{,
		otherwise}\\
	\end{cases}
\end{equation*}
\end{definition}
\begin{definition}
    The \underline{infimum} can be defined as
    \begin{equation*}
        \inf_{x \in S} f(x) = - \sup_{x \in S} -f(x)
    \end{equation*}
\end{definition}
So now by replacing maximization problems with the supremum (and minimization
problems with infimum), we would never fall into the case where ``an optimal
solution doesn't exist'' as long as we consider the supremum (or infimum). In
other words, if the problem is feasible and unbounded, we can always find an
optimal solution.

\begin{definition}
	A set $S\subseteq\R^n$ is \underline{convex} if $t\in[0,1]$ and for any $x,y \in
	S$, $tx + (1-t)y \in S$.
\end{definition}
\begin{definition}
	For some set $S$, $f:S \to \R$ is \underline{convex} if $t\in[0,1]$ and for
	any $x,y \in S$, $f(tx + (1-t)y) \leq tf(x) + (1-t)f(y)$.
\end{definition}

We can begin to define a few types of optimization problems:

\begin{enumerate}
    \item {\bf Linear Programming}
        \begin{equation*}
			f(x) = c^Tx \text{ and } S = \{x\in\R^n:Ax\leq B\}
            \text{ where } A \in\R^{m\times n}, b\in\R^m\text{ and } c \in \R^n
        \end{equation*}
	\item {\bf Integer Linear Programming}
		\begin{equation*}
			f(x) = c^T \text{ and } S = \{ x \in \Z^n:Ax\leq B\}
            \text{ where } A \in\R^{m\times n}, b\in\R^m\text{ and } c \in \R^n
		\end{equation*}
	\item {\bf Convex Optimization}
		\begin{equation*}
			S \subseteq \R^n \text{ is convex and } f:S \to \R \text{ is convex
		}
		\end{equation*}
\end{enumerate}
\begin{definition}
	The \underline{convex hull} of $S \subseteq \R^n$, denoted by $\conv{S}$ is
	the minimal convex set that contains $S$.
\end{definition}
\begin{remark}
	The convex hull of $S$ is unique.
\end{remark}

Consider the optimization problem $\min(f(x):x\in S)$ where $S\subseteq R^n$ and
$f:\R^n\to\R$. We can ``reduce'' this problem to  a convex optimization problem
with a linear objective function.

{\bf \underline{Step 1}:} Linearize the objective function. Let
\begin{equation*}
	\hat{S} = \{(x,y) : x \in S, y = f(x)\} \subseteq \R^{n+1}
\end{equation*}
Then,
\begin{equation*}
	\min(f(x):x\in S) = \min(y:(x,y) \in \hat{S})
\end{equation*}

{\bf \underline{Step 2}:} Convexify $S$.
If $f:\R^n\to\R$ is linear, $\min(f(x):x\in S) = \min(f(x):x\in \conv{S})$.

\begin{remark}
	This is true from a theoretical point of view, but it is not very practical
	in application. Taking the convex hull doesn't produce any better solutions.
\end{remark}
\end{document}
